{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7594a51b-b85f-4a4f-80ea-e02f787250f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ecc6870-5623-4b2c-8527-25e2fae1a6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30)\n",
      "(143, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
    " random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc068750-8d5d-445e-842d-6d8bd47679ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "MinMaxScaler(copy=True, feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69e1ff57-93e0-480b-8956-dcf82293fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't print using scientific notation\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "# transform data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "# transform test data\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "009d432a-aa2c-451c-b55d-9658a3c71575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9440559440559441\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
    " random_state=0)\n",
    "svm = SVC(C=100)\n",
    "svm.fit(X_train, y_train)\n",
    "print(svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a188eca-65e3-4f26-9240-c31e669c8e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeccff26-57fe-4223-b5d8-6ae389ccb40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 57.  ,  60.33,  78.  , ...,  17.33,  16.67,  22.67],\n",
       "        [ 66.67,  80.67,  88.67, ...,   8.67,   8.33,   9.33],\n",
       "        [ 18.67,  14.33,  15.  , ...,  22.67,  22.67,  25.67],\n",
       "        ...,\n",
       "        [ 77.33,  90.33,  93.33, ..., 233.67, 234.33, 230.67],\n",
       "        [ 87.67,  71.67,  63.33, ...,  96.  ,  92.33,  96.33],\n",
       "        [ 91.  ,  90.67,  94.67, ...,  40.33,  42.33,  42.33]],\n",
       "       dtype=float32),\n",
       " 'images': array([[[ 57.  ,  60.33,  78.  , ...,  40.67,  36.67,  34.  ],\n",
       "         [ 52.67,  60.  ,  92.  , ...,  46.67,  36.  ,  30.  ],\n",
       "         [ 46.  ,  66.  , 113.33, ...,  51.  ,  43.33,  37.67],\n",
       "         ...,\n",
       "         [ 61.  ,  62.67,  62.33, ...,  19.  ,  18.33,  18.67],\n",
       "         [ 62.67,  64.67,  67.  , ...,  17.33,  18.  ,  22.33],\n",
       "         [ 65.33,  68.  ,  67.  , ...,  17.33,  16.67,  22.67]],\n",
       " \n",
       "        [[ 66.67,  80.67,  88.67, ...,  81.33,  78.67,  70.67],\n",
       "         [ 66.33,  78.  ,  83.33, ...,  81.  ,  79.  ,  75.33],\n",
       "         [ 69.33,  80.67,  77.67, ...,  80.33,  79.33,  78.  ],\n",
       "         ...,\n",
       "         [ 45.  ,  44.  ,  39.67, ...,   8.  ,   7.  ,   7.67],\n",
       "         [ 45.67,  44.  ,  40.67, ...,   8.  ,   7.67,   7.67],\n",
       "         [ 44.67,  42.67,  39.33, ...,   8.67,   8.33,   9.33]],\n",
       " \n",
       "        [[ 18.67,  14.33,  15.  , ...,  19.  ,  15.67,  10.  ],\n",
       "         [ 21.  ,  20.  ,  17.67, ...,  19.  ,  14.67,  11.  ],\n",
       "         [ 26.  ,  23.33,  19.67, ...,  21.67,  17.67,  14.33],\n",
       "         ...,\n",
       "         [116.33, 136.  , 142.  , ...,  23.67,  21.67,  22.67],\n",
       "         [ 92.33, 112.33, 132.33, ...,  26.  ,  24.  ,  24.67],\n",
       "         [ 84.  ,  98.67, 120.  , ...,  22.67,  22.67,  25.67]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 77.33,  90.33,  93.33, ...,  80.  ,  87.67,  79.67],\n",
       "         [ 84.33,  96.67,  97.  , ...,  67.33,  83.67,  81.67],\n",
       "         [ 92.  ,  99.67,  94.  , ...,  66.  ,  81.  ,  79.  ],\n",
       "         ...,\n",
       "         [ 27.  ,  27.67,  29.67, ..., 233.33, 228.33, 220.33],\n",
       "         [ 21.33,  25.  ,  28.67, ..., 233.67, 232.33, 225.  ],\n",
       "         [ 24.67,  28.67,  31.  , ..., 233.67, 234.33, 230.67]],\n",
       " \n",
       "        [[ 87.67,  71.67,  63.33, ...,  46.  ,  45.67,  47.33],\n",
       "         [ 85.33,  72.  ,  65.67, ...,  51.33,  50.67,  51.67],\n",
       "         [ 83.  ,  71.33,  68.33, ...,  57.  ,  57.  ,  58.  ],\n",
       "         ...,\n",
       "         [220.67, 219.67, 217.67, ...,  91.  ,  90.  ,  89.  ],\n",
       "         [220.  , 219.67, 218.33, ...,  93.33,  91.33,  93.33],\n",
       "         [219.67, 220.33, 219.33, ...,  96.  ,  92.33,  96.33]],\n",
       " \n",
       "        [[ 91.  ,  90.67,  94.67, ...,  99.33,  90.  ,  69.67],\n",
       "         [ 86.  ,  83.  ,  89.  , ...,  96.67,  93.33,  74.  ],\n",
       "         [ 80.  ,  80.  ,  89.67, ...,  96.33,  98.  ,  85.  ],\n",
       "         ...,\n",
       "         [ 37.67,  34.67,  29.67, ...,  36.  ,  37.33,  43.67],\n",
       "         [ 39.67,  35.67,  30.67, ...,  38.  ,  39.  ,  40.33],\n",
       "         [ 41.67,  36.67,  31.67, ...,  40.33,  42.33,  42.33]]],\n",
       "       dtype=float32),\n",
       " 'target': array([61, 25,  9, ..., 14, 15, 14], dtype=int64),\n",
       " 'target_names': array(['Alejandro Toledo', 'Alvaro Uribe', 'Amelie Mauresmo',\n",
       "        'Andre Agassi', 'Angelina Jolie', 'Ariel Sharon',\n",
       "        'Arnold Schwarzenegger', 'Atal Bihari Vajpayee', 'Bill Clinton',\n",
       "        'Carlos Menem', 'Colin Powell', 'David Beckham', 'Donald Rumsfeld',\n",
       "        'George Robertson', 'George W Bush', 'Gerhard Schroeder',\n",
       "        'Gloria Macapagal Arroyo', 'Gray Davis', 'Guillermo Coria',\n",
       "        'Hamid Karzai', 'Hans Blix', 'Hugo Chavez', 'Igor Ivanov',\n",
       "        'Jack Straw', 'Jacques Chirac', 'Jean Chretien',\n",
       "        'Jennifer Aniston', 'Jennifer Capriati', 'Jennifer Lopez',\n",
       "        'Jeremy Greenstock', 'Jiang Zemin', 'John Ashcroft',\n",
       "        'John Negroponte', 'Jose Maria Aznar', 'Juan Carlos Ferrero',\n",
       "        'Junichiro Koizumi', 'Kofi Annan', 'Laura Bush',\n",
       "        'Lindsay Davenport', 'Lleyton Hewitt', 'Luiz Inacio Lula da Silva',\n",
       "        'Mahmoud Abbas', 'Megawati Sukarnoputri', 'Michael Bloomberg',\n",
       "        'Naomi Watts', 'Nestor Kirchner', 'Paul Bremer', 'Pete Sampras',\n",
       "        'Recep Tayyip Erdogan', 'Ricardo Lagos', 'Roh Moo-hyun',\n",
       "        'Rudolph Giuliani', 'Saddam Hussein', 'Serena Williams',\n",
       "        'Silvio Berlusconi', 'Tiger Woods', 'Tom Daschle', 'Tom Ridge',\n",
       "        'Tony Blair', 'Vicente Fox', 'Vladimir Putin', 'Winona Ryder'],\n",
       "       dtype='<U25'),\n",
       " 'DESCR': \".. _labeled_faces_in_the_wild_dataset:\\n\\nThe Labeled Faces in the Wild face recognition dataset\\n------------------------------------------------------\\n\\nThis dataset is a collection of JPEG pictures of famous people collected\\nover the internet, all details are available on the official website:\\n\\n    http://vis-www.cs.umass.edu/lfw/\\n\\nEach picture is centered on a single face. The typical task is called\\nFace Verification: given a pair of two pictures, a binary classifier\\nmust predict whether the two images are from the same person.\\n\\nAn alternative task, Face Recognition or Face Identification is:\\ngiven the picture of the face of an unknown person, identify the name\\nof the person by referring to a gallery of previously seen pictures of\\nidentified persons.\\n\\nBoth Face Verification and Face Recognition are tasks that are typically\\nperformed on the output of a model trained to perform Face Detection. The\\nmost popular model for Face Detection is called Viola-Jones and is\\nimplemented in the OpenCV library. The LFW faces were extracted by this\\nface detector from various online websites.\\n\\n**Data Set Characteristics:**\\n\\n    =================   =======================\\n    Classes                                5749\\n    Samples total                         13233\\n    Dimensionality                         5828\\n    Features            real, between 0 and 255\\n    =================   =======================\\n\\nUsage\\n~~~~~\\n\\n``scikit-learn`` provides two loaders that will automatically download,\\ncache, parse the metadata files, decode the jpeg and convert the\\ninteresting slices into memmapped numpy arrays. This dataset size is more\\nthan 200 MB. The first load typically takes more than a couple of minutes\\nto fully decode the relevant part of the JPEG files into numpy arrays. If\\nthe dataset has  been loaded once, the following times the loading times\\nless than 200ms by using a memmapped version memoized on the disk in the\\n``~/scikit_learn_data/lfw_home/`` folder using ``joblib``.\\n\\nThe first loader is used for the Face Identification task: a multi-class\\nclassification task (hence supervised learning)::\\n\\n  >>> from sklearn.datasets import fetch_lfw_people\\n  >>> lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\\n\\n  >>> for name in lfw_people.target_names:\\n  ...     print(name)\\n  ...\\n  Ariel Sharon\\n  Colin Powell\\n  Donald Rumsfeld\\n  George W Bush\\n  Gerhard Schroeder\\n  Hugo Chavez\\n  Tony Blair\\n\\nThe default slice is a rectangular shape around the face, removing\\nmost of the background::\\n\\n  >>> lfw_people.data.dtype\\n  dtype('float32')\\n\\n  >>> lfw_people.data.shape\\n  (1288, 1850)\\n\\n  >>> lfw_people.images.shape\\n  (1288, 50, 37)\\n\\nEach of the ``1140`` faces is assigned to a single person id in the ``target``\\narray::\\n\\n  >>> lfw_people.target.shape\\n  (1288,)\\n\\n  >>> list(lfw_people.target[:10])\\n  [5, 6, 3, 1, 0, 1, 3, 4, 3, 0]\\n\\nThe second loader is typically used for the face verification task: each sample\\nis a pair of two picture belonging or not to the same person::\\n\\n  >>> from sklearn.datasets import fetch_lfw_pairs\\n  >>> lfw_pairs_train = fetch_lfw_pairs(subset='train')\\n\\n  >>> list(lfw_pairs_train.target_names)\\n  ['Different persons', 'Same person']\\n\\n  >>> lfw_pairs_train.pairs.shape\\n  (2200, 2, 62, 47)\\n\\n  >>> lfw_pairs_train.data.shape\\n  (2200, 5828)\\n\\n  >>> lfw_pairs_train.target.shape\\n  (2200,)\\n\\nBoth for the :func:`sklearn.datasets.fetch_lfw_people` and\\n:func:`sklearn.datasets.fetch_lfw_pairs` function it is\\npossible to get an additional dimension with the RGB color channels by\\npassing ``color=True``, in that case the shape will be\\n``(2200, 2, 62, 47, 3)``.\\n\\nThe :func:`sklearn.datasets.fetch_lfw_pairs` datasets is subdivided into\\n3 subsets: the development ``train`` set, the development ``test`` set and\\nan evaluation ``10_folds`` set meant to compute performance metrics using a\\n10-folds cross validation scheme.\\n\\n.. topic:: References:\\n\\n * `Labeled Faces in the Wild: A Database for Studying Face Recognition\\n   in Unconstrained Environments.\\n   <http://vis-www.cs.umass.edu/lfw/lfw.pdf>`_\\n   Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.\\n   University of Massachusetts, Amherst, Technical Report 07-49, October, 2007.\\n\\n\\nExamples\\n~~~~~~~~\\n\\n:ref:`sphx_glr_auto_examples_applications_plot_face_recognition.py`\\n\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba017d2b-dc2c-4790-989d-1e12b3cb8dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alejandro Toledo           39 0\n",
      "Alvaro Uribe               35 1\n",
      "Amelie Mauresmo            21 2\n",
      "\n",
      "Andre Agassi               36 3\n",
      "Angelina Jolie             20 4\n",
      "Ariel Sharon               77 5\n",
      "\n",
      "Arnold Schwarzenegger      42 6\n",
      "Atal Bihari Vajpayee       24 7\n",
      "Bill Clinton               29 8\n",
      "\n",
      "Carlos Menem               21 9\n",
      "Colin Powell              236 10\n",
      "David Beckham              31 11\n",
      "\n",
      "Donald Rumsfeld           121 12\n",
      "George Robertson           22 13\n",
      "George W Bush             530 14\n",
      "\n",
      "Gerhard Schroeder         109 15\n",
      "Gloria Macapagal Arroyo    44 16\n",
      "Gray Davis                 26 17\n",
      "\n",
      "Guillermo Coria            30 18\n",
      "Hamid Karzai               22 19\n",
      "Hans Blix                  39 20\n",
      "\n",
      "Hugo Chavez                71 21\n",
      "Igor Ivanov                20 22\n",
      "Jack Straw                 28 23\n",
      "\n",
      "Jacques Chirac             52 24\n",
      "Jean Chretien              55 25\n",
      "Jennifer Aniston           21 26\n",
      "\n",
      "Jennifer Capriati          42 27\n",
      "Jennifer Lopez             21 28\n",
      "Jeremy Greenstock          24 29\n",
      "\n",
      "Jiang Zemin                20 30\n",
      "John Ashcroft              53 31\n",
      "John Negroponte            31 32\n",
      "\n",
      "Jose Maria Aznar           23 33\n",
      "Juan Carlos Ferrero        28 34\n",
      "Junichiro Koizumi          60 35\n",
      "\n",
      "Kofi Annan                 32 36\n",
      "Laura Bush                 41 37\n",
      "Lindsay Davenport          22 38\n",
      "\n",
      "Lleyton Hewitt             41 39\n",
      "Luiz Inacio Lula da Silva  48 40\n",
      "Mahmoud Abbas              29 41\n",
      "\n",
      "Megawati Sukarnoputri      33 42\n",
      "Michael Bloomberg          20 43\n",
      "Naomi Watts                22 44\n",
      "\n",
      "Nestor Kirchner            37 45\n",
      "Paul Bremer                20 46\n",
      "Pete Sampras               22 47\n",
      "\n",
      "Recep Tayyip Erdogan       30 48\n",
      "Ricardo Lagos              27 49\n",
      "Roh Moo-hyun               32 50\n",
      "\n",
      "Rudolph Giuliani           26 51\n",
      "Saddam Hussein             23 52\n",
      "Serena Williams            52 53\n",
      "\n",
      "Silvio Berlusconi          33 54\n",
      "Tiger Woods                23 55\n",
      "Tom Daschle                25 56\n",
      "\n",
      "Tom Ridge                  33 57\n",
      "Tony Blair                144 58\n",
      "Vicente Fox                32 59\n",
      "\n",
      "Vladimir Putin             49 60\n",
      "Winona Ryder               24 61\n"
     ]
    }
   ],
   "source": [
    "counts = np.bincount(people.target)\n",
    "# print counts next to target names:\n",
    "for i, (count, name) in enumerate(zip(counts, people.target_names)):\n",
    "    print(\"{0:25} {1:3}\".format(name, count), i)\n",
    "    if (i + 1) % 3 == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9598dc4e-995c-4c40-8eac-a06643380885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[ 57.  ,  60.33,  78.  , ...,  17.33,  16.67,  22.67],\n",
      "       [ 66.67,  80.67,  88.67, ...,   8.67,   8.33,   9.33],\n",
      "       [ 18.67,  14.33,  15.  , ...,  22.67,  22.67,  25.67],\n",
      "       ...,\n",
      "       [ 77.33,  90.33,  93.33, ..., 233.67, 234.33, 230.67],\n",
      "       [ 87.67,  71.67,  63.33, ...,  96.  ,  92.33,  96.33],\n",
      "       [ 91.  ,  90.67,  94.67, ...,  40.33,  42.33,  42.33]],\n",
      "      dtype=float32), 'images': array([[[ 57.  ,  60.33,  78.  , ...,  40.67,  36.67,  34.  ],\n",
      "        [ 52.67,  60.  ,  92.  , ...,  46.67,  36.  ,  30.  ],\n",
      "        [ 46.  ,  66.  , 113.33, ...,  51.  ,  43.33,  37.67],\n",
      "        ...,\n",
      "        [ 61.  ,  62.67,  62.33, ...,  19.  ,  18.33,  18.67],\n",
      "        [ 62.67,  64.67,  67.  , ...,  17.33,  18.  ,  22.33],\n",
      "        [ 65.33,  68.  ,  67.  , ...,  17.33,  16.67,  22.67]],\n",
      "\n",
      "       [[ 66.67,  80.67,  88.67, ...,  81.33,  78.67,  70.67],\n",
      "        [ 66.33,  78.  ,  83.33, ...,  81.  ,  79.  ,  75.33],\n",
      "        [ 69.33,  80.67,  77.67, ...,  80.33,  79.33,  78.  ],\n",
      "        ...,\n",
      "        [ 45.  ,  44.  ,  39.67, ...,   8.  ,   7.  ,   7.67],\n",
      "        [ 45.67,  44.  ,  40.67, ...,   8.  ,   7.67,   7.67],\n",
      "        [ 44.67,  42.67,  39.33, ...,   8.67,   8.33,   9.33]],\n",
      "\n",
      "       [[ 18.67,  14.33,  15.  , ...,  19.  ,  15.67,  10.  ],\n",
      "        [ 21.  ,  20.  ,  17.67, ...,  19.  ,  14.67,  11.  ],\n",
      "        [ 26.  ,  23.33,  19.67, ...,  21.67,  17.67,  14.33],\n",
      "        ...,\n",
      "        [116.33, 136.  , 142.  , ...,  23.67,  21.67,  22.67],\n",
      "        [ 92.33, 112.33, 132.33, ...,  26.  ,  24.  ,  24.67],\n",
      "        [ 84.  ,  98.67, 120.  , ...,  22.67,  22.67,  25.67]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 77.33,  90.33,  93.33, ...,  80.  ,  87.67,  79.67],\n",
      "        [ 84.33,  96.67,  97.  , ...,  67.33,  83.67,  81.67],\n",
      "        [ 92.  ,  99.67,  94.  , ...,  66.  ,  81.  ,  79.  ],\n",
      "        ...,\n",
      "        [ 27.  ,  27.67,  29.67, ..., 233.33, 228.33, 220.33],\n",
      "        [ 21.33,  25.  ,  28.67, ..., 233.67, 232.33, 225.  ],\n",
      "        [ 24.67,  28.67,  31.  , ..., 233.67, 234.33, 230.67]],\n",
      "\n",
      "       [[ 87.67,  71.67,  63.33, ...,  46.  ,  45.67,  47.33],\n",
      "        [ 85.33,  72.  ,  65.67, ...,  51.33,  50.67,  51.67],\n",
      "        [ 83.  ,  71.33,  68.33, ...,  57.  ,  57.  ,  58.  ],\n",
      "        ...,\n",
      "        [220.67, 219.67, 217.67, ...,  91.  ,  90.  ,  89.  ],\n",
      "        [220.  , 219.67, 218.33, ...,  93.33,  91.33,  93.33],\n",
      "        [219.67, 220.33, 219.33, ...,  96.  ,  92.33,  96.33]],\n",
      "\n",
      "       [[ 91.  ,  90.67,  94.67, ...,  99.33,  90.  ,  69.67],\n",
      "        [ 86.  ,  83.  ,  89.  , ...,  96.67,  93.33,  74.  ],\n",
      "        [ 80.  ,  80.  ,  89.67, ...,  96.33,  98.  ,  85.  ],\n",
      "        ...,\n",
      "        [ 37.67,  34.67,  29.67, ...,  36.  ,  37.33,  43.67],\n",
      "        [ 39.67,  35.67,  30.67, ...,  38.  ,  39.  ,  40.33],\n",
      "        [ 41.67,  36.67,  31.67, ...,  40.33,  42.33,  42.33]]],\n",
      "      dtype=float32), 'target': array([61, 25,  9, ..., 14, 15, 14], dtype=int64), 'target_names': array(['Alejandro Toledo', 'Alvaro Uribe', 'Amelie Mauresmo',\n",
      "       'Andre Agassi', 'Angelina Jolie', 'Ariel Sharon',\n",
      "       'Arnold Schwarzenegger', 'Atal Bihari Vajpayee', 'Bill Clinton',\n",
      "       'Carlos Menem', 'Colin Powell', 'David Beckham', 'Donald Rumsfeld',\n",
      "       'George Robertson', 'George W Bush', 'Gerhard Schroeder',\n",
      "       'Gloria Macapagal Arroyo', 'Gray Davis', 'Guillermo Coria',\n",
      "       'Hamid Karzai', 'Hans Blix', 'Hugo Chavez', 'Igor Ivanov',\n",
      "       'Jack Straw', 'Jacques Chirac', 'Jean Chretien',\n",
      "       'Jennifer Aniston', 'Jennifer Capriati', 'Jennifer Lopez',\n",
      "       'Jeremy Greenstock', 'Jiang Zemin', 'John Ashcroft',\n",
      "       'John Negroponte', 'Jose Maria Aznar', 'Juan Carlos Ferrero',\n",
      "       'Junichiro Koizumi', 'Kofi Annan', 'Laura Bush',\n",
      "       'Lindsay Davenport', 'Lleyton Hewitt', 'Luiz Inacio Lula da Silva',\n",
      "       'Mahmoud Abbas', 'Megawati Sukarnoputri', 'Michael Bloomberg',\n",
      "       'Naomi Watts', 'Nestor Kirchner', 'Paul Bremer', 'Pete Sampras',\n",
      "       'Recep Tayyip Erdogan', 'Ricardo Lagos', 'Roh Moo-hyun',\n",
      "       'Rudolph Giuliani', 'Saddam Hussein', 'Serena Williams',\n",
      "       'Silvio Berlusconi', 'Tiger Woods', 'Tom Daschle', 'Tom Ridge',\n",
      "       'Tony Blair', 'Vicente Fox', 'Vladimir Putin', 'Winona Ryder'],\n",
      "      dtype='<U25'), 'DESCR': \".. _labeled_faces_in_the_wild_dataset:\\n\\nThe Labeled Faces in the Wild face recognition dataset\\n------------------------------------------------------\\n\\nThis dataset is a collection of JPEG pictures of famous people collected\\nover the internet, all details are available on the official website:\\n\\n    http://vis-www.cs.umass.edu/lfw/\\n\\nEach picture is centered on a single face. The typical task is called\\nFace Verification: given a pair of two pictures, a binary classifier\\nmust predict whether the two images are from the same person.\\n\\nAn alternative task, Face Recognition or Face Identification is:\\ngiven the picture of the face of an unknown person, identify the name\\nof the person by referring to a gallery of previously seen pictures of\\nidentified persons.\\n\\nBoth Face Verification and Face Recognition are tasks that are typically\\nperformed on the output of a model trained to perform Face Detection. The\\nmost popular model for Face Detection is called Viola-Jones and is\\nimplemented in the OpenCV library. The LFW faces were extracted by this\\nface detector from various online websites.\\n\\n**Data Set Characteristics:**\\n\\n    =================   =======================\\n    Classes                                5749\\n    Samples total                         13233\\n    Dimensionality                         5828\\n    Features            real, between 0 and 255\\n    =================   =======================\\n\\nUsage\\n~~~~~\\n\\n``scikit-learn`` provides two loaders that will automatically download,\\ncache, parse the metadata files, decode the jpeg and convert the\\ninteresting slices into memmapped numpy arrays. This dataset size is more\\nthan 200 MB. The first load typically takes more than a couple of minutes\\nto fully decode the relevant part of the JPEG files into numpy arrays. If\\nthe dataset has  been loaded once, the following times the loading times\\nless than 200ms by using a memmapped version memoized on the disk in the\\n``~/scikit_learn_data/lfw_home/`` folder using ``joblib``.\\n\\nThe first loader is used for the Face Identification task: a multi-class\\nclassification task (hence supervised learning)::\\n\\n  >>> from sklearn.datasets import fetch_lfw_people\\n  >>> lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\\n\\n  >>> for name in lfw_people.target_names:\\n  ...     print(name)\\n  ...\\n  Ariel Sharon\\n  Colin Powell\\n  Donald Rumsfeld\\n  George W Bush\\n  Gerhard Schroeder\\n  Hugo Chavez\\n  Tony Blair\\n\\nThe default slice is a rectangular shape around the face, removing\\nmost of the background::\\n\\n  >>> lfw_people.data.dtype\\n  dtype('float32')\\n\\n  >>> lfw_people.data.shape\\n  (1288, 1850)\\n\\n  >>> lfw_people.images.shape\\n  (1288, 50, 37)\\n\\nEach of the ``1140`` faces is assigned to a single person id in the ``target``\\narray::\\n\\n  >>> lfw_people.target.shape\\n  (1288,)\\n\\n  >>> list(lfw_people.target[:10])\\n  [5, 6, 3, 1, 0, 1, 3, 4, 3, 0]\\n\\nThe second loader is typically used for the face verification task: each sample\\nis a pair of two picture belonging or not to the same person::\\n\\n  >>> from sklearn.datasets import fetch_lfw_pairs\\n  >>> lfw_pairs_train = fetch_lfw_pairs(subset='train')\\n\\n  >>> list(lfw_pairs_train.target_names)\\n  ['Different persons', 'Same person']\\n\\n  >>> lfw_pairs_train.pairs.shape\\n  (2200, 2, 62, 47)\\n\\n  >>> lfw_pairs_train.data.shape\\n  (2200, 5828)\\n\\n  >>> lfw_pairs_train.target.shape\\n  (2200,)\\n\\nBoth for the :func:`sklearn.datasets.fetch_lfw_people` and\\n:func:`sklearn.datasets.fetch_lfw_pairs` function it is\\npossible to get an additional dimension with the RGB color channels by\\npassing ``color=True``, in that case the shape will be\\n``(2200, 2, 62, 47, 3)``.\\n\\nThe :func:`sklearn.datasets.fetch_lfw_pairs` datasets is subdivided into\\n3 subsets: the development ``train`` set, the development ``test`` set and\\nan evaluation ``10_folds`` set meant to compute performance metrics using a\\n10-folds cross validation scheme.\\n\\n.. topic:: References:\\n\\n * `Labeled Faces in the Wild: A Database for Studying Face Recognition\\n   in Unconstrained Environments.\\n   <http://vis-www.cs.umass.edu/lfw/lfw.pdf>`_\\n   Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.\\n   University of Massachusetts, Amherst, Technical Report 07-49, October, 2007.\\n\\n\\nExamples\\n~~~~~~~~\\n\\n:ref:`sphx_glr_auto_examples_applications_plot_face_recognition.py`\\n\"}\n"
     ]
    }
   ],
   "source": [
    "print(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "93a334eb-879a-42c4-8de3-27bb8a4a597b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AxesSubplot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-d918362f80ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(axes.ravel)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeople\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAxesSubplot\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m  \u001b[1;31m#ax.set_title(people.target_names[target])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#plt.suptitle(\"some_faces\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AxesSubplot' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#from sklearn.datasets import fetch_lfw_people\n",
    "#people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\n",
    "image_shape = people.images[0].shape\n",
    "fix, axes = plt.subplots(2, 5, figsize=(15, 8), subplot_kw={'xticks': (), 'yticks': ()})\n",
    "#print(axes.ravel)\n",
    "for image in zip(people.images):\n",
    "    axes[AxesSubplot].imshow(image)\n",
    " #ax.set_title(people.target_names[target])\n",
    "#plt.suptitle(\"some_faces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c3d7eb5-e351-4ae9-bcf3-7abf73388115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<AxesSubplot:>, <AxesSubplot:>, <AxesSubplot:>, <AxesSubplot:>,\n",
       "       <AxesSubplot:>, <AxesSubplot:>, <AxesSubplot:>, <AxesSubplot:>,\n",
       "       <AxesSubplot:>, <AxesSubplot:>], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prinaxes.ravel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4411f-3d57-482c-9906-94d3670688d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
